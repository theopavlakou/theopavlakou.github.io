{
 "metadata": {
  "name": "",
  "signature": "sha256:c55127e6a161162af27b1122633422cd3947f130d3296ae6a492ffea0959634d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The EM Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The EM Algorithm is used when we would like to do maximum likelihood (or MAP) estimation but where our model has hidden variables i.e. variables that we cannot observe but that we believe are involved in the generation of the data. For instance, it may be the case that we believe that there is a correlation between people having eczema and asthma, but we may not believe that any of these two causes the other. We may instead believe that they are both caused by the presence of some allele in a persons DNA (by the way, I do not claim to know anything about biology or genetics so take this with a grain of salt). The presence of this could be the latent variable, but we may never see this in most people. \n",
      "\n",
      "In this case, we have the visible variables, lets denote them as $x$, and the hidden variables, denoted by $z$. We would like to maximise the *marginal* likelihood over the visible variables i.e. we want to solve the following \n",
      "\n",
      "$$\n",
      "\\theta^* = \\arg \\max_{\\theta} \\log p(x|\\theta) = \\arg \\max_{\\theta} \\log \\sum_{z} p(x, z|\\theta).\n",
      "$$\n",
      "\n",
      "\n",
      "Due to the presence of the summation in the log, this is actually a very difficult problem to solve. We cannot solve for it directly. Instead, the usual way to solve this is somthing called the *EM algorithm* or the *Expectation Maximisation algorithm*. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The EM algorithm can be derived in the following way. We want to maximise the log likelihood of the visible data i.e. \n",
      "\n",
      "$$\n",
      "l(\\theta) = \\log \\sum_{z} p(x, z|\\theta), \n",
      "$$\n",
      "\n",
      "for each datapoint $x$ in the data set, so we do this very neat, unintuitive trick. We introduce a distribution over the hidden variables, $q(z)$\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "l(\\theta) &= \\log \\left(\\sum_{z} q(z) \\frac{p(x, z|\\theta)}{q(z)}\\right)\n",
      "\\\\\n",
      "&\\geq \\sum_{z} q(z) \\log\\left(\\frac{p(x, z|\\theta)}{q(z)}\\right).\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "Now there are two ways to view this equation. Since this will turn out to be an iterative algorithm, we will call the parameters at iteration $t$, $\\theta^t$. At iteration $t$ we then have that\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "l(\\theta^t) &\\geq \\sum_{z} q(z) \\log\\left(\\frac{p(x, z|\\theta^t)}{q(z)}\\right)\n",
      "\\\\\n",
      "&= \\sum_{z} q(z) \\log \\frac{p(z|x, \\theta^t)}{q(z)} + \\log p(x|\\theta^t) \n",
      "\\\\\n",
      "&= -\\text{KL}\\left(q(z)|| p(z|x, \\theta^t) \\right) + \\log p(x|\\theta^t).\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "We know that the KL divergence is always non-negative and it is zero when $q(z) = p(z|x, \\theta^t)$. Therefore, we maximise this by setting these as equal. When this is the case, the lower bound on the log-likelihood of the visible data is tight i.e. there is equality. We can also view this in another way as well. Let's rewrite the above (after setting $q(z) = p(z|x, \\theta^t)$) as\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "l(\\theta^t) &= \\sum_{z} p(z | x, \\theta^t) \\log \\left(p(x, z|\\theta^t) \\right) + H(p(z | x, \\theta^t) )\n",
      "\\\\\n",
      "&= \\mathbb{E}_{p(z | x, \\theta^t)} \\left[ \\log \\left( p(x, z| \\theta^t) \\right) \\right] + H( p(z | x, \\theta^t) ). \n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "Now, if we allow the parameters of $p(x, z | \\theta^t)$ to be free, but still keep $q(z) = p(z | x, \\theta^t)$, then we get the following\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "l(\\theta) &\\geq \\sum_{z} p(z | x, \\theta^t) \\log \\left(p(x, z|\\theta \\right) + H(p(z | x, \\theta^t) )\n",
      "\\\\\n",
      "&= \\mathbb{E}_{p(z | x, \\theta^t)} \\left[ \\log \\left( p(x, z| \\theta) \\right) \\right] + H( p(z | x, \\theta^t) ). \n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "We can then maximise the right hand side with respect to $\\theta$ (remember, we fix $\\theta^t$). To do this, we don't really need to take the entropy ($H(p(z | x, \\theta^t))$) into account, since it is not a function of $\\theta$. Let us define an auxiliary function \n",
      "\n",
      "$$\n",
      "Q(\\theta, \\theta^t) = \\mathbb{E}_{p(z | x, \\theta^t)} \\left[ \\log p(x, z|\\theta) \\right].\n",
      "$$\n",
      "\n",
      "We then maximise this with respect to $\\theta$ i.e. \n",
      "\n",
      "$$\n",
      "\\theta^{t+1} = \\arg \\max_{\\theta} Q(\\theta, \\theta^t)\n",
      "$$\n",
      "\n",
      "then we repeat. So, basically, the EM algorithm iterates over two steps. At iteration $t$ we make the lower bound tight, which we do by setting $q(z) = p(z|x, \\theta^t)$. This is needed to take the *expected* log likelihood over the visible data, which is why it is called the Expectation step. We then *maximise* this with respect to $\\theta$, which is why this is called the Maximisation step. We can prove that this always is guaranteed to increase the log likelihood of the visible data (until it converges). This is because\n",
      "\n",
      "$$\n",
      "l(\\theta^t) = Q(\\theta, \\theta^t) + H(p(z | x, \\theta^t)) \\leq Q(\\theta^{t+1}, \\theta^t) + H(p(z | x, \\theta^t)) = -\\text{KL}\\left(p(z|x, \\theta^t)|| p(z|x, \\theta^{t+1}) \\right) + \\log p(x|\\theta^{t+1}) \\leq l(\\theta^{t+1})\n",
      "$$\n",
      "which means that $l(\\theta^t)\\leq l(\\theta^{t+1})$, for all $t$. This means that at each iteration of the EM algorithm, the parameters become better explanations of the data (if we are doing maximum likelihood) or at least do not become worse. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The catch\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The EM algorithm only guarantees that we will reach a *local* optimium. This means that there may have been better parameters to increase the likelihood but because it guarantees that it will never decrease, it will never reach them (because to get there it would have to temporarily decrease). For this reason, it is sometimes useful to do the algorithm a couple of times starting from different initial parameters and then choose the one that maximises the likelihood (or use cross validation). \n",
      "\n",
      "However, it works well in practice and it is used extensively in machine learning and statistics. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}